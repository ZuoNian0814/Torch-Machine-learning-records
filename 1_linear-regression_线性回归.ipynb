{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **任务一 线性回归 Linear Regression**\n",
    "\n",
    "我不准备记录底层的数学逻辑部分的学习，这个项目仅仅只是为了展示我在代码上的理解。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **torch线性层（全连接层）**\n",
    "\n",
    "torch库最简单的线性映射层，最直观的表现是它能将不同形状的数据以某种规律扩展或压缩维度。\n",
    "\n",
    "如果不考虑数学因素，对于这么简单的结构其实没有太多需要记录的。\n",
    "\n",
    "### **使用方法**\n",
    "\n",
    "它来自于 `torch` 的 `nn` 子类中，通常我们取nn作为别名方便调用 `nn.Linear(input_dim, output_dim)` 方法返回我们需要的线性层对象。\n",
    "\n",
    "将张量直接作为参数，传入给线性层对象，把它当作一个函数使用即可实现前向传播过程，如以下代码展示\n",
    "\n",
    "#### 1.1 **特征值张量**\n",
    "\n",
    "形状为 **[batch_size, dim]** 的输入张量，batch维度是为了方便批量计算额外添加的维度，对于计算是必要的，哪怕每个批次只有一个数据 **[1, dim]**。"
   ],
   "id": "1698b189658d1f70"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-01T12:55:03.007590Z",
     "start_time": "2026-01-01T12:55:03.004235Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 8\n",
    "input_tensor = torch.rand((batch_size, 3))\n",
    "\n",
    "# 定义线性层 nn.Linear\n",
    "Linear_layer = nn.Linear(3, 5)\n",
    "\n",
    "# 前向传播过程\n",
    "output_tensor = Linear_layer(input_tensor)\n",
    "\n",
    "print('input_tensor:', input_tensor.shape)\n",
    "print('output_tensor:', output_tensor.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor: torch.Size([8, 3])\n",
      "output_tensor: torch.Size([8, 5])\n"
     ]
    }
   ],
   "execution_count": 406
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "我们注意到对于形状为 **[batch, dim]** 的数据，它会以dim作为被映射的维度进行特征长度的变化。\n",
    "\n",
    "接下来尝试使用其他常见的张量形状\n",
    "\n",
    "#### 1.2 **图像张量**\n",
    "\n",
    "形状为 **[batch_size, channel, height, width]** 的输入张量，channel是通道维度，也就是图像不同颜色的通道，对于不同模式的图像有不同情况。\n",
    "\n",
    "| 图像模式 | 图像类型       | 通道维度长度 |\n",
    "|------|------------|--------|\n",
    "| L    | 灰度图像       | 1      |\n",
    "| RGB  | 彩色图像       | 3      |\n",
    "| RGBA | 带透明通道的彩色图像 | 4      |\n",
    "\n",
    "实际上会有更多模式，我常用的是这三种，实际上RGBA也不太常用，不过不重要，对应场景等理解之后可以随机应变。\n"
   ],
   "id": "eb446435a92b60b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:55:03.023891Z",
     "start_time": "2026-01-01T12:55:03.020469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 8\n",
    "input_tensor = torch.rand((batch_size, 3, 8, 8))\n",
    "\n",
    "# 定义线性层 nn.Linear\n",
    "Linear_layer = nn.Linear(8, 16)\n",
    "\n",
    "# 前向传播过程\n",
    "output_tensor = Linear_layer(input_tensor)\n",
    "\n",
    "print('input_tensor:', input_tensor.shape)\n",
    "print('output_tensor:', output_tensor.shape)"
   ],
   "id": "e6ae8cee49f1a3ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor: torch.Size([8, 3, 8, 8])\n",
      "output_tensor: torch.Size([8, 3, 8, 16])\n"
     ]
    }
   ],
   "execution_count": 407
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "对于比较常见的图像类型数据 **[batch, channel, height, width]** 它只能从width维度进行变化\n",
    "\n",
    "看起来似乎拉伸的图像的宽度，但实际上线性数值计算这个过程并没有考虑空间特征，所以不用担心这种变换会影响对图像的处理，因为它从一开始就不该这么用。\n",
    "\n",
    "在图像中如何使用线性层也是很常见的，但至少不该像这样，后面记录图像任务的时候会再解释。\n",
    "\n",
    "#### 1.3 **序列张量**"
   ],
   "id": "94daaf205bbe8764"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:55:03.045940Z",
     "start_time": "2026-01-01T12:55:03.042258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 8\n",
    "input_tensor = torch.rand((batch_size, 5, 8))\n",
    "\n",
    "# 定义线性层 nn.Linear\n",
    "Linear_layer = nn.Linear(8, 16)\n",
    "\n",
    "# 前向传播过程\n",
    "output_tensor = Linear_layer(input_tensor)\n",
    "\n",
    "print('input_tensor:', input_tensor.shape)\n",
    "print('output_tensor:', output_tensor.shape)"
   ],
   "id": "27305d9c22032522",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor: torch.Size([8, 5, 8])\n",
      "output_tensor: torch.Size([8, 5, 16])\n"
     ]
    }
   ],
   "execution_count": 408
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "对于序列数据 **[batch, seq, dim]** 它的变化看起来比图像合理的多，将特征维度进行了映射，同时没有改变其序列长度。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **模型定义**\n",
    "\n",
    "对于一个完整的模型通常封装起来会更方便使用和定义，官方方法是使用class类用法实现，以下为定义示例。"
   ],
   "id": "d91e23027c2a4ffd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:55:03.064772Z",
     "start_time": "2026-01-01T12:55:03.061940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Linear_layer = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Linear_layer(x)"
   ],
   "id": "825d271fb0d35c6c",
   "outputs": [],
   "execution_count": 409
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在这里不再强调类用法如何使用，仅对模型定义做出一些说明。\n",
    "\n",
    "将模型继承自 `torch.nn.Module` 类中，由于 `nn`已经取了别名，所以这里直接使用 `nn.Module`\n",
    "\n",
    "将需要的层定义在初始化方法中，将前向传播过程定义在forward方法中，forward方法名不能改变，这是官方文档规定的。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **模型调用**\n",
    "\n",
    "同样是将实例化之后的模型对象直接当作函数前向传播，这与 `torch` 的任何一个层的用法都一样，本质上每个层都可以理解为一个继承自 `nn.Module` 的子类，我们只不过自定义了一个"
   ],
   "id": "d9c4ecdd7fa165a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:55:03.079766Z",
     "start_time": "2026-01-01T12:55:03.076328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 8\n",
    "input_tensor = torch.rand((batch_size, 3))\n",
    "\n",
    "# 实例化模型对象\n",
    "model = Model(3, 5)\n",
    "\n",
    "# 前向传播过程\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "print('model:\\n', model)\n",
    "print('input_tensor:', input_tensor.shape)\n",
    "print('output_tensor:', output_tensor.shape)"
   ],
   "id": "360b69005c21c99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:\n",
      " Model(\n",
      "  (Linear_layer): Linear(in_features=3, out_features=5, bias=True)\n",
      ")\n",
      "input_tensor: torch.Size([8, 3])\n",
      "output_tensor: torch.Size([8, 5])\n"
     ]
    }
   ],
   "execution_count": 410
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 4. **线性回归的训练**\n",
    "\n",
    "介绍完第一个需要认识的层之后可以开始关于它的第一个任务——线性回归\n",
    "\n",
    "先前学习过线性回归，这个任务是深度学习的基础，所以从它来引入深度学习框架的使用再合适不过\n",
    "\n",
    "### 4.1 **数据生成**\n",
    "\n",
    "本次任务我们需要拟合的是一条直线，实际上只需要两组“特征值”数据，分别是 $y = wx + b$ 的 $x$ 和 $y$\n",
    "\n",
    "其中 $w$ 和 $b$ 就是模型需要训练得到的模型参数。"
   ],
   "id": "d0ec1666e4d6b83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:55:03.102768Z",
     "start_time": "2026-01-01T12:55:03.098600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w, b, noise_range = 2.3, -8, 0.1\n",
    "batch_size = 100\n",
    "# 输入 x\n",
    "x = torch.randint(low=-50, high=51, size=(batch_size, 1), dtype=torch.float32)\n",
    "\n",
    "# 输出 y\n",
    "y = w * x + b\n",
    "# 噪声\n",
    "noise = 2 * (torch.rand_like(y) * noise_range) - 1\n",
    "\n",
    "# 4. 整数叠加噪声，得到最终结果\n",
    "y = y + noise\n",
    "\n",
    "x.shape, y.shape"
   ],
   "id": "b73f8c4fc8688678",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 1]), torch.Size([100, 1]))"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 411
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这里我们生成了200个随机数据x，然后使用 $y = kx + b$ 计算出 $y$ 作为输出\n",
    "\n",
    "其中 $x$ 作为 input 也就是输入，$y$ 作为 label 也就是标注数据。这里的强调主要是方便理解常见的深度学习命名方法。\n",
    "\n",
    "### 4.2 **定义模型、损失函数和优化器**\n",
    "\n",
    "学习机器学习时了解到，线性回归任务通常使用均方误差函数，torch框架中正好有封装这种损失函数 `nn.MSELoss()`。\n",
    "\n",
    "优化器的类别很多，这里用比较常用的，具体有哪些优化器它们有什么区别这里不强调，因为只是为了展示不同任务在模型结构和层用法。\n",
    "\n",
    "lr 就是我们线性回归时用到的学习率。"
   ],
   "id": "ccdc0a8eaab8f4da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:55:03.130262Z",
     "start_time": "2026-01-01T12:55:03.127133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 线性回归的输入和输出均只有一个维度\n",
    "model = Model(1, 1)\n",
    "# 定义损失函数，这里使用均方误差损失（MSELoss）\n",
    "criterion = nn.MSELoss()\n",
    "# 定义优化器，使用随机梯度下降（SGD）来更新权重和偏置\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0008)"
   ],
   "id": "820339d2a4582dd1",
   "outputs": [],
   "execution_count": 412
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.3 **迭代训练**\n",
    "\n",
    "流程与线性回归几乎无异。需要注意的是对于 `torch` 框架很多功能都是自动的，并且需要按照其定义的代码段逐个执行。\n",
    "\n",
    "- 前向传播: `output = model(x)`\n",
    "- 计算损失: `loss = criterion(output, y)`\n",
    "- 梯度清零: `optimizer.zero_grad()`\n",
    "- 反向传播: `loss.backward()`\n",
    "- 更新权重和偏置: `optimizer.step()`\n",
    "\n",
    "最后根据具体情况增加必要的指标，方便实时监测训练情况。"
   ],
   "id": "f941f16e82495089"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:55:03.272743Z",
     "start_time": "2026-01-01T12:55:03.142987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 1000\n",
    "for epoch in range(1000):\n",
    "    # 前向传播，得到预测值\n",
    "    output = model(x)\n",
    "    # 计算损失\n",
    "    loss = criterion(output, y)\n",
    "    # 梯度清零，因为在每次反向传播前都要清除之前累积的梯度\n",
    "    optimizer.zero_grad()\n",
    "    # 反向传播，计算梯度\n",
    "    loss.backward()\n",
    "    # 更新权重和偏置\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'[epoch {epoch+1}]loss:', loss.item())\n"
   ],
   "id": "c8beec01b6e2f77f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 50]loss: 77.56211853027344\n",
      "[epoch 100]loss: 66.16356658935547\n",
      "[epoch 150]loss: 56.440242767333984\n",
      "[epoch 200]loss: 48.14592361450195\n",
      "[epoch 250]loss: 41.070560455322266\n",
      "[epoch 300]loss: 35.035037994384766\n",
      "[epoch 350]loss: 29.886518478393555\n",
      "[epoch 400]loss: 25.494667053222656\n",
      "[epoch 450]loss: 21.74828338623047\n",
      "[epoch 500]loss: 18.5524845123291\n",
      "[epoch 550]loss: 15.826355934143066\n",
      "[epoch 600]loss: 13.500873565673828\n",
      "[epoch 650]loss: 11.517169952392578\n",
      "[epoch 700]loss: 9.825000762939453\n",
      "[epoch 750]loss: 8.381522178649902\n",
      "[epoch 800]loss: 7.150174140930176\n",
      "[epoch 850]loss: 6.099789619445801\n",
      "[epoch 900]loss: 5.203781604766846\n",
      "[epoch 950]loss: 4.439455986022949\n",
      "[epoch 1000]loss: 3.787461042404175\n"
     ]
    }
   ],
   "execution_count": 413
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "模型损失值逐步降低，至少说明模型在训练集中正在进行有效的学习。\n",
    "\n",
    "为什么强调训练集？涉及到有关**过拟合**的情况，这里同样不做过多记录。\n",
    "\n",
    "### 4.4 **测试模型**\n",
    "\n",
    "误差在可接受的范围内"
   ],
   "id": "cfa2141065619bcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:55:03.283537Z",
     "start_time": "2026-01-01T12:55:03.279326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "x1 = torch.randint(low=-100, high=101, size=(1, 1), dtype=torch.float32)\n",
    "x2 = torch.randint(low=-100, high=101, size=(1, 1), dtype=torch.float32)\n",
    "\n",
    "y_p1 = model(x1)\n",
    "y_p2 = model(x2)\n",
    "y1 = w * x1 + b\n",
    "y2 = w * x2 + b\n",
    "\n",
    "print('真实值1:', y1.item())\n",
    "print('预测值1:', y_p1.item())\n",
    "\n",
    "print('真实值2:', y2.item())\n",
    "print('预测值2:', y_p2.item())\n"
   ],
   "id": "1e0a8c87c0b808bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实值1: -217.3000030517578\n",
      "预测值1: -216.7677764892578\n",
      "真实值2: 148.39999389648438\n",
      "预测值2: 149.8332977294922\n"
     ]
    }
   ],
   "execution_count": 414
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.5 观察模型参数\n",
    "\n",
    "这里获取模型的权重 $w$ 和偏置 $b$，发现模型虽然仍存在些许误差，但总体在往目标直线拟合。"
   ],
   "id": "50f7a30983a94fdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T12:55:03.296780Z",
     "start_time": "2026-01-01T12:55:03.293830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weight = model.Linear_layer.weight.data  # 获取权重\n",
    "bias = model.Linear_layer.bias.data      # 获取偏置\n",
    "print(f\"训练权重(w): {weight.item()}, 训练偏置(b): {bias.item()}\")\n",
    "print(f\"真实权重(w): {w}, 真实偏置(b): {b}\")"
   ],
   "id": "e2b20574f62812ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练权重(w): 2.3056671619415283, 训练偏置(b): -6.952065944671631\n",
      "真实权重(w): 2.3, 真实偏置(b): -8\n"
     ]
    }
   ],
   "execution_count": 415
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. **总结**\n",
    "\n",
    "这是第一个关于 `torch` 框架的任务，足够简单也足够好理解，很好的将机器学习的线性回归引入的深度学习框架的使用。"
   ],
   "id": "d4be1ff7534decf3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
